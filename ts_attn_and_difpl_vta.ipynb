{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MARYA5VjkKhv",
        "outputId": "32892e4e-3920-40bb-915a-2648a401f333"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/世界モデル_最終課題 /timeseries.csv\")\n",
        "\n",
        "ID_COL = \"patient_id\"\n",
        "TIME_COL = \"time_days\"\n",
        "\n",
        "feature_cols = [\n",
        "    c for c in df.columns\n",
        "    if c not in [ID_COL, TIME_COL]\n",
        "    and not c.startswith(\"met_\")\n",
        "    and not c.startswith(\"tx_\")\n",
        "]\n",
        "\n",
        "F = len(feature_cols)\n",
        "print(\"num features:\", F)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "def make_triples(df):\n",
        "    \"\"\"\n",
        "    (x_t, x_{t+1}) でグラフを作って x_{t+2} を予測するためのサンプルを作る\n",
        "    \"\"\"\n",
        "    triples = []\n",
        "    for pid, g in df.groupby(ID_COL):\n",
        "        g = g.sort_values(TIME_COL)\n",
        "        x = g[feature_cols].values\n",
        "        t = g[TIME_COL].values\n",
        "\n",
        "        # t, t+1, t+2 が必要なので -2 まで\n",
        "        for i in range(len(g) - 2):\n",
        "            triples.append({\n",
        "                \"pid\": pid,\n",
        "                \"x_t\": x[i],\n",
        "                \"x_tp1\": x[i+1],\n",
        "                \"x_tp2\": x[i+2],\n",
        "                \"dt01\": float(t[i+1] - t[i]),\n",
        "                \"dt12\": float(t[i+2] - t[i+1]),\n",
        "            })\n",
        "    return triples\n",
        "\n",
        "triples = make_triples(df)\n",
        "print(\"num triples:\", len(triples))\n",
        "\n",
        "for tr in triples:\n",
        "    tr[\"x_t\"]   = torch.tensor(tr[\"x_t\"],   dtype=torch.float, device=device)\n",
        "    tr[\"x_tp1\"] = torch.tensor(tr[\"x_tp1\"], dtype=torch.float, device=device)\n",
        "    tr[\"x_tp2\"] = torch.tensor(tr[\"x_tp2\"], dtype=torch.float, device=device)\n",
        "    tr[\"dt01\"]  = torch.tensor(tr[\"dt01\"],  dtype=torch.float, device=device)\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as torch_F\n",
        "\n",
        "class CrossTimeAttention(nn.Module):\n",
        "    def __init__(self, in_dim, attn_dim):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(in_dim, attn_dim, bias=False)\n",
        "        self.k = nn.Linear(in_dim, attn_dim, bias=False)\n",
        "\n",
        "    def forward(self, x_t, x_tp1):\n",
        "        \"\"\"\n",
        "        x_t, x_tp1: (F, in_dim)\n",
        "        \"\"\"\n",
        "        Q = self.q(x_t)\n",
        "        K = self.k(x_tp1)\n",
        "        A = Q @ K.T / np.sqrt(Q.size(-1))\n",
        "        return torch_F.softmax(A, dim=1)  # row-wise\n",
        "\n",
        "!pip -q install torch-geometric\n",
        "\n",
        "from torch_geometric.nn import GCNConv, dense_diff_pool\n",
        "\n",
        "class DiffPoolItemNet(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, num_clusters):\n",
        "        super().__init__()\n",
        "        self.gnn_embed = GCNConv(in_dim, hidden_dim)\n",
        "        self.gnn_pool  = GCNConv(in_dim, num_clusters)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # dense adj -> edge_index/edge_weight\n",
        "        edge_index = adj.nonzero().T\n",
        "        edge_weight = adj[edge_index[0], edge_index[1]]\n",
        "\n",
        "        z = self.gnn_embed(x, edge_index, edge_weight)\n",
        "        s = self.gnn_pool(x, edge_index, edge_weight)\n",
        "\n",
        "        x_pool, adj_pool, _, _ = dense_diff_pool(\n",
        "            z.unsqueeze(0),\n",
        "            adj.unsqueeze(0),\n",
        "            s.unsqueeze(0)\n",
        "        )\n",
        "        return x_pool.squeeze(0), adj_pool.squeeze(0), s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSd1TyiOvgb2",
        "outputId": "5d3ddeb9-0d28-41ed-d04d-b936457bd377"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "num features: 42\n",
            "num triples: 123742\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemGraphEncoder(nn.Module):\n",
        "    def __init__(self, num_clusters, F_features):\n",
        "        super().__init__()\n",
        "        self.F_features = F_features\n",
        "        self.attn = CrossTimeAttention(1, 16)\n",
        "        self.diffpool = DiffPoolItemNet(4, 16, num_clusters) # Reduced hidden_dim from 32 to 16\n",
        "        self.latent_dim = num_clusters * 16   # Adjusted latent_dim accordingly\n",
        "\n",
        "    def forward(self, triple):\n",
        "        x_t   = torch.tensor(triple[\"x_t\"],   dtype=torch.float, device=device)\n",
        "        x_tp1 = torch.tensor(triple[\"x_tp1\"], dtype=torch.float, device=device)\n",
        "        dt01  = torch.tensor(triple[\"dt01\"],  dtype=torch.float, device=device)\n",
        "\n",
        "        A = self.attn(x_t.unsqueeze(1), x_tp1.unsqueeze(1))\n",
        "\n",
        "        H = torch.stack([\n",
        "            x_t,\n",
        "            x_tp1,\n",
        "            x_tp1 - x_t,\n",
        "            torch.ones(self.F_features, device=device) * dt01,\n",
        "        ], dim=1)\n",
        "\n",
        "        x_pool, _, S = self.diffpool(H, A)\n",
        "        z = x_pool.flatten()   # ← ここが GRU に入る\n",
        "        return z, S"
      ],
      "metadata": {
        "id": "THuYdFgMQRI_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatientGRUModel(nn.Module):\n",
        "    def __init__(self, latent_dim, F):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=128,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.Linear(128, F)\n",
        "\n",
        "    def forward(self, z_seq):\n",
        "        # z_seq: (1, T-2, latent_dim)\n",
        "        out, _ = self.gru(z_seq)\n",
        "        h_last = out[:, -1]\n",
        "        return self.decoder(h_last).squeeze(0)\n"
      ],
      "metadata": {
        "id": "kKFiGMMhQTD5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "triples_by_pid = defaultdict(list)\n",
        "for tr in triples:\n",
        "    triples_by_pid[tr[\"pid\"]].append(tr)\n",
        "\n",
        "for tr in triples:\n",
        "    tr[\"x_t\"]   = torch.tensor(tr[\"x_t\"],   dtype=torch.float, device=device)\n",
        "    tr[\"x_tp1\"] = torch.tensor(tr[\"x_tp1\"], dtype=torch.float, device=device)\n",
        "    tr[\"x_tp2\"] = torch.tensor(tr[\"x_tp2\"], dtype=torch.float, device=device)\n",
        "    tr[\"dt01\"]  = torch.tensor(tr[\"dt01\"],  dtype=torch.float, device=device)\n"
      ],
      "metadata": {
        "id": "SobbjCJeQUhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7d0420-1c70-4b97-a8d0-48def25bc716"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3458526560.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tr[\"x_t\"]   = torch.tensor(tr[\"x_t\"],   dtype=torch.float, device=device)\n",
            "/tmp/ipython-input-3458526560.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tr[\"x_tp1\"] = torch.tensor(tr[\"x_tp1\"], dtype=torch.float, device=device)\n",
            "/tmp/ipython-input-3458526560.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tr[\"x_tp2\"] = torch.tensor(tr[\"x_tp2\"], dtype=torch.float, device=device)\n",
            "/tmp/ipython-input-3458526560.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tr[\"dt01\"]  = torch.tensor(tr[\"dt01\"],  dtype=torch.float, device=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = ItemGraphEncoder(num_clusters=3, F_features=F).to(device) # Reduced num_clusters from 5 to 3\n",
        "gru_model = PatientGRUModel(\n",
        "    latent_dim=encoder.latent_dim,\n",
        "    F=F\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(encoder.parameters()) + list(gru_model.parameters()),\n",
        "    lr=1e-3\n",
        ")\n"
      ],
      "metadata": {
        "id": "3x_EVEAOQV6J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CausalTransformerIntervention(nn.Module):\n",
        "    def __init__(self, latent_dim, K=3, d_model=128, nhead=4, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.K = K\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.in_proj = nn.Linear(latent_dim, d_model)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,\n",
        "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "\n",
        "        # 次の潜在表現を予測（自己教師）\n",
        "        self.next_head = nn.Linear(d_model, latent_dim)\n",
        "\n",
        "        # 介入状態（離散）ヘッド\n",
        "        self.u_head = nn.Linear(d_model, K)\n",
        "\n",
        "    def forward(self, z_seq, key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        z_seq: (B, T, D)\n",
        "        key_padding_mask: (B, T) True=pad  (可変長バッチにする場合)\n",
        "        \"\"\"\n",
        "        B, T, D = z_seq.shape\n",
        "        x = self.in_proj(z_seq)  # (B,T,d_model)\n",
        "\n",
        "        # causal mask: 未来を見ない\n",
        "        causal_mask = torch.triu(torch.ones(T, T, device=z_seq.device), diagonal=1).bool()\n",
        "\n",
        "        h = self.encoder(x, mask=causal_mask, src_key_padding_mask=key_padding_mask)  # (B,T,d_model)\n",
        "\n",
        "        pred_z_next = self.next_head(h)  # (B,T,D) ただし t→t+1 用に使う\n",
        "        logits_u = self.u_head(h)        # (B,T,K)\n",
        "        q_u = F.softmax(logits_u, dim=-1)\n",
        "\n",
        "        return pred_z_next, q_u, logits_u\n"
      ],
      "metadata": {
        "id": "qAAi1KxTQycu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_loss(pred_z_next, z_seq, key_padding_mask=None):\n",
        "    # pred at t should match z_{t+1}\n",
        "    pred = pred_z_next[:, :-1, :]\n",
        "    target = z_seq[:, 1:, :]\n",
        "\n",
        "    if key_padding_mask is None:\n",
        "        return F.mse_loss(pred, target)\n",
        "    else:\n",
        "        # pad を除外した MSE\n",
        "        mask = (~key_padding_mask[:, 1:]).unsqueeze(-1)  # (B,T-1,1)\n",
        "        se = (pred - target)**2\n",
        "        se = se * mask\n",
        "        return se.sum() / (mask.sum() * pred.size(-1) + 1e-8)\n"
      ],
      "metadata": {
        "id": "sbJjTGwKOR1V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tv_kl_loss(q_u, key_padding_mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    sum_t KL(q_t || q_{t-1})\n",
        "    q_u: (B,T,K)\n",
        "    \"\"\"\n",
        "    q1 = q_u[:, 1:, :]\n",
        "    q0 = q_u[:, :-1, :]\n",
        "\n",
        "    kl = (q1 * (torch.log(q1 + eps) - torch.log(q0 + eps))).sum(dim=-1)  # (B,T-1)\n",
        "\n",
        "    if key_padding_mask is None:\n",
        "        return kl.mean()\n",
        "    else:\n",
        "        # t=1..T-1 の有効部分\n",
        "        mask = (~key_padding_mask[:, 1:]).float()\n",
        "        return (kl * mask).sum() / (mask.sum() + 1e-8)\n"
      ],
      "metadata": {
        "id": "qOw9Us1_OTbH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy_reg(q_u, key_padding_mask=None, eps=1e-8):\n",
        "    ent = -(q_u * torch.log(q_u + eps)).sum(dim=-1)  # (B,T)\n",
        "\n",
        "    if key_padding_mask is None:\n",
        "        return -ent.mean()  # \"最大化\"したいのでマイナス\n",
        "    else:\n",
        "        mask = (~key_padding_mask).float()\n",
        "        return -(ent * mask).sum() / (mask.sum() + 1e-8)\n"
      ],
      "metadata": {
        "id": "G2LGyd_qOVHv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 3\n",
        "vta = CausalTransformerIntervention(latent_dim=encoder.latent_dim, K=K).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(encoder.parameters()) + list(vta.parameters()),\n",
        "    lr=1e-3\n",
        ")\n",
        "\n",
        "num_epochs = 10\n",
        "lambda_tv = 0.2\n",
        "lambda_ent = 0.01\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    vta.train()\n",
        "\n",
        "    total = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for pid, patient_triples in triples_by_pid.items():\n",
        "        if len(patient_triples) < 3:\n",
        "            continue\n",
        "\n",
        "        # z を作る（今と同じ）\n",
        "        z_list = []\n",
        "        for tr in patient_triples:\n",
        "            z, _ = encoder(tr)\n",
        "            z_list.append(z)\n",
        "\n",
        "        z_seq = torch.stack(z_list).unsqueeze(0)  # (1,T,D)\n",
        "\n",
        "        pred_z_next, q_u, _ = vta(z_seq)\n",
        "\n",
        "        loss_pred = prediction_loss(pred_z_next, z_seq)\n",
        "        loss_tv   = tv_kl_loss(q_u)\n",
        "        loss_ent  = entropy_reg(q_u)\n",
        "\n",
        "        loss = loss_pred + lambda_tv * loss_tv + lambda_ent * loss_ent\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss_pred.item()\n",
        "        n += 1\n",
        "\n",
        "    print(f\"epoch {epoch}: pred_loss={total/max(n,1):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PUTodaBOWrf",
        "outputId": "a91c2d7a-fe87-4cf4-f011-7285b8e39a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-129526605.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_t   = torch.tensor(triple[\"x_t\"],   dtype=torch.float, device=device)\n",
            "/tmp/ipython-input-129526605.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_tp1 = torch.tensor(triple[\"x_tp1\"], dtype=torch.float, device=device)\n",
            "/tmp/ipython-input-129526605.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dt01  = torch.tensor(triple[\"dt01\"],  dtype=torch.float, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: pred_loss=1459.2570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "some_pid = 1001\n",
        "vta.eval()\n",
        "encoder.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # ある患者 pid の z_seq を作る\n",
        "    patient_triples = triples_by_pid[some_pid]\n",
        "    z_list = [encoder(tr)[0] for tr in patient_triples]\n",
        "    z_seq = torch.stack(z_list).unsqueeze(0)\n",
        "\n",
        "    _, q_u, _ = vta(z_seq)\n",
        "    u_hat = q_u[0].argmax(dim=-1).cpu().numpy()  # (T,)"
      ],
      "metadata": {
        "id": "o_fDUzKtPWPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segments_from_labels(labels):\n",
        "    segs = []\n",
        "    s = 0\n",
        "    cur = labels[0]\n",
        "    for i in range(1, len(labels)):\n",
        "        if labels[i] != cur:\n",
        "            segs.append((s, i-1, int(cur)))\n",
        "            s = i\n",
        "            cur = labels[i]\n",
        "    segs.append((s, len(labels)-1, int(cur)))\n",
        "    return segs\n",
        "\n",
        "segs = segments_from_labels(u_hat)\n",
        "print(segs)  # (start_idx, end_idx, regime)"
      ],
      "metadata": {
        "id": "EgS-uoYWSIy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}